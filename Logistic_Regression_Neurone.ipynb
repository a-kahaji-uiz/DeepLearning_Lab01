{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression Logistique avec une mentalité de Réseau de Neurones\n",
    "\n",
    "Bienvenue dans votre premier devoir de programmation (obligatoire) ! Vous allez construire un classifieur par régression logistique pour reconnaître des chats. Ce devoir vous guidera étape par étape pour réaliser cela avec une \"mentalité\" de réseau de neurones, ce qui affinera également vos intuitions sur le Deep Learning.\n",
    "\n",
    "**Instructions :**\n",
    "- N'utilisez pas de boucles (for/while) dans votre code, sauf si les instructions vous le demandent explicitement.\n",
    "- Utilisez `np.dot(X,Y)` pour calculer les produits scalaires.\n",
    "\n",
    "**Vous apprendrez à :**\n",
    "- Construire l'architecture générale d'un algorithme d'apprentissage, y compris :\n",
    "    - L'initialisation des paramètres\n",
    "    - Le calcul de la fonction de coût et de son gradient\n",
    "    - L'utilisation d'un algorithme d'optimisation (descente de gradient) \n",
    "- Rassembler les trois fonctions ci-dessus dans une fonction modèle principale, dans le bon ordre.\n",
    "\n",
    "\n",
    "Avant de soumettre votre devoir, assurez-vous de ne pas faire ce qui suit :\n",
    "\n",
    "1. Vous n'avez pas ajouté d'instructions `print` _supplémentaires_ dans le devoir.\n",
    "2. Vous n'avez pas ajouté de cellules de code _supplémentaires_.\n",
    "3. Vous n'avez pas modifié les paramètres des fonctions.\n",
    "4. Vous n'utilisez pas de variables globales à l'intérieur de vos exercices notés.\n",
    "5. Vous ne modifiez pas le code du devoir là où ce n'est pas requis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table des Matières\n",
    "- [1 - Packages](#1)\n",
    "- [2 - Vue d'ensemble du problème](#2)\n",
    "    - [Exercice 1](#ex-1)\n",
    "    - [Exercice 2](#ex-2)\n",
    "- [3 - Architecture générale de l'algorithme d'apprentissage](#3)\n",
    "- [4 - Construction des parties de notre algorithme](#4)\n",
    "    - [4.1 - Fonctions auxiliaires](#4-1)\n",
    "        - [Exercice 3 - sigmoid](#ex-3)\n",
    "    - [4.2 - Initialisation des paramètres](#4-2)\n",
    "        - [Exercice 4 - initialize_with_zeros](#ex-4)\n",
    "    - [4.3 - Propagation avant et arrière](#4-3)\n",
    "        - [Exercice 5 - propagate](#ex-5)\n",
    "    - [4.4 - Optimisation](#4-4)\n",
    "        - [Exercice 6 - optimize](#ex-6)\n",
    "        - [Exercice 7 - predict](#ex-7)\n",
    "- [5 - Fusionner toutes les fonctions dans un modèle](#5)\n",
    "    - [Exercice 8 - model](#ex-8)\n",
    "- [6 - Analyse approfondie (optionnel/non noté)](#6)\n",
    "- [7 - Test avec votre propre image (optionnel/non noté)](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages ##\n",
    "\n",
    "Tout d'abord, exécutons la cellule ci-dessous pour importer tous les packages dont vous aurez besoin lors de ce devoir. \n",
    "- [numpy](https://numpy.org/doc/1.20/) est le package fondamental pour le calcul scientifique avec Python.\n",
    "- [h5py](http://www.h5py.org) est un package courant pour interagir avec un ensemble de données stocké dans un fichier H5.\n",
    "- [matplotlib](http://matplotlib.org) est une bibliothèque célèbre pour tracer des graphiques en Python.\n",
    "- [PIL](https://pillow.readthedocs.io/en/stable/) et [scipy](https://www.scipy.org/) sont utilisés ici pour tester votre modèle avec votre propre image à la fin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### v1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from lr_utils import load_dataset\n",
    "from public_tests import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Vue d'ensemble du problème ##\n",
    "\n",
    "**Énoncé du problème** : On vous donne un jeu de données (\"data.h5\") contenant :\n",
    "    - un ensemble d'entraînement de m_train images étiquetées comme chat (y=1) ou non-chat (y=0)\n",
    "    - un ensemble de test de m_test images étiquetées comme chat ou non-chat\n",
    "    - chaque image est de forme (num_px, num_px, 3) où 3 correspond aux 3 canaux (RGB). Ainsi, chaque image est carrée (hauteur = num_px) et (largeur = num_px).\n",
    "\n",
    "Vous allez construire un algorithme simple de reconnaissance d'image capable de classer correctement les images comme chat ou non-chat.\n",
    "\n",
    "Familiarisons-nous davantage avec le jeu de données. Chargez les données en exécutant le code suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données (chat/non-chat)\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons ajouté \"_orig\" à la fin des jeux de données d'images (train et test) car nous allons les prétraiter. Après le prétraitement, nous obtiendrons train_set_x et test_set_x (les étiquettes train_set_y et test_set_y n'ont besoin d'aucun prétraitement).\n",
    "\n",
    "Chaque ligne de votre train_set_x_orig et test_set_x_orig est un tableau représentant une image. Vous pouvez visualiser un exemple en exécutant le code suivant. N'hésitez pas à changer la valeur de `index` et à relancer pour voir d'autres images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'une image\n",
    "index = 25\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print (\"y = \" + str(train_set_y[:, index]) + \", c'est une image de '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nombreux bugs logiciels en Deep Learning proviennent de dimensions de matrices/vecteurs qui ne correspondent pas. Si vous parvenez à maintenir vos dimensions de matrice/vecteur correctes, vous éliminerez une grande partie des bugs. \n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercice 1\n",
    "Trouvez les valeurs pour :\n",
    "    - m_train (nombre d'exemples d'entraînement)\n",
    "    - m_test (nombre d'exemples de test)\n",
    "    - num_px (= hauteur = largeur d'une image d'entraînement)\n",
    "Rappelez-vous que `train_set_x_orig` est un tableau numpy de forme (m_train, num_px, num_px, 3). Par exemple, vous pouvez accéder à `m_train` en écrivant `train_set_x_orig.shape[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "921fe679a632ec7ec9963069fa405725",
     "grade": false,
     "grade_id": "cell-c4e7e9c1f174eb83",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#(≈ 3 lignes de code)\n",
    "# m_train = \n",
    "# m_test = \n",
    "# num_px = \n",
    "# VOTRE CODE COMMENCE ICI\n",
    "\n",
    "\n",
    "# VOTRE CODE SE TERMINE ICI\n",
    "\n",
    "print (\"Nombre d'exemples d'entraînement : m_train = \" + str(m_train))\n",
    "print (\"Nombre d'exemples de test : m_test = \" + str(m_test))\n",
    "print (\"Hauteur/Largeur de chaque image : num_px = \" + str(num_px))\n",
    "print (\"Chaque image est de taille : (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"forme de train_set_x : \" + str(train_set_x_orig.shape))\n",
    "print (\"forme de train_set_y : \" + str(train_set_y.shape))\n",
    "print (\"forme de test_set_x : \" + str(test_set_x_orig.shape))\n",
    "print (\"forme de test_set_y : \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu pour m_train, m_test et num_px** : \n",
    "<table style=\"width:15%\">\n",
    "  <tr>\n",
    "    <td> m_train </td>\n",
    "    <td> 209 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>m_test</td>\n",
    "    <td> 50 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>num_px</td>\n",
    "    <td> 64 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour plus de commodité, vous devez maintenant redimensionner les images de forme (num_px, num_px, 3) en un tableau numpy de forme (num_px $*$ num_px $*$ 3, 1). Après cela, notre jeu de données d'entraînement (et de test) sera un tableau numpy où chaque colonne représente une image aplatie. Il devrait y avoir m_train (respectivement m_test) colonnes.\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercice 2\n",
    "Redimensionnez les jeux de données d'entraînement et de test de sorte que les images de taille (num_px, num_px, 3) soient aplaties en vecteurs uniques de forme (num\\_px $*$ num\\_px $*$ 3, 1).\n",
    "\n",
    "Une astuce lorsque vous voulez aplatir une matrice X de forme (a,b,c,d) en une matrice X_flatten de forme (b$*$c$*$d, a) est d'utiliser : \n",
    "```python\n",
    "X_flatten = X.reshape(X.shape[0], -1).T      # X.T est la transposée de X\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a2aa62bdd8c01450111b758ef159aec",
     "grade": false,
     "grade_id": "cell-0f43921062c34e50",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Redimensionner les exemples d'entraînement et de test\n",
    "#(≈ 2 lignes de code)\n",
    "# train_set_x_flatten = ...\n",
    "# test_set_x_flatten = ...\n",
    "# VOTRE CODE COMMENCE ICI\n",
    "\n",
    "\n",
    "# VOTRE CODE SE TERMINE ICI\n",
    "\n",
    "# Vérification que les 10 premiers pixels de la deuxième image sont au bon endroit\n",
    "assert np.alltrue(train_set_x_flatten[0:10, 1] == [196, 192, 190, 193, 186, 182, 188, 179, 174, 213]), \"Mauvaise solution. Utilisez (X.shape[0], -1).T.\"\n",
    "assert np.alltrue(test_set_x_flatten[0:10, 1] == [115, 110, 111, 137, 129, 129, 155, 146, 145, 159]), \"Mauvaise solution. Utilisez (X.shape[0], -1).T.\"\n",
    "\n",
    "print (\"forme de train_set_x_flatten : \" + str(train_set_x_flatten.shape))\n",
    "print (\"forme de train_set_y : \" + str(train_set_y.shape))\n",
    "print (\"forme de test_set_x_flatten : \" + str(test_set_x_flatten.shape))\n",
    "print (\"forme de test_set_y : \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**: \n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td>train_set_x_flatten shape</td>\n",
    "    <td> (12288, 209)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>train_set_y shape</td>\n",
    "    <td>(1, 209)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>test_set_x_flatten shape</td>\n",
    "    <td>(12288, 50)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>test_set_y shape</td>\n",
    "    <td>(1, 50)</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour représenter des images couleur, les canaux rouge, vert et bleu (RGB) doivent être spécifiés pour chaque pixel, et donc la valeur du pixel est en réalité un vecteur de trois nombres allant de 0 à 255.\n",
    "\n",
    "Une étape de prétraitement courante en apprentissage automatique consiste à centrer et standardiser votre jeu de données, ce qui signifie que vous soustrayez la moyenne de tout le tableau numpy à chaque exemple, puis divisez chaque exemple par l'écart type de tout le tableau numpy. Mais pour les jeux de données d'images, il est plus simple, plus pratique et presque aussi efficace de simplement diviser chaque ligne du jeu de données par 255 (la valeur maximale d'un canal de pixel).\n",
    "\n",
    "Standardisons notre jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = train_set_x_flatten / 255.\n",
    "test_set_x = test_set_x_flatten / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "    \n",
    "**Ce qu'il faut retenir :**\n",
    "\n",
    "Les étapes communes pour le prétraitement d'un nouveau jeu de données sont :\n",
    "- Comprendre les dimensions et les formes du problème (m_train, m_test, num_px, ...)\n",
    "- Redimensionner les jeux de données de telle sorte que chaque exemple soit maintenant un vecteur de taille (num_px \\* num_px \\* 3, 1)\n",
    "- \"Standardiser\" les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Architecture générale de l'algorithme d'apprentissage ##\n",
    "\n",
    "Il est temps de concevoir un algorithme simple pour distinguer les images de chats des images sans chats.\n",
    "\n",
    "Vous allez construire une Régression Logistique, en utilisant une mentalité de Réseau de Neurones. La figure suivante explique pourquoi **la Régression Logistique est en fait un Réseau de Neurones très simple !**\n",
    "\n",
    "<img src=\"images/LogReg_kiank.png\" style=\"width:650px;height:400px;\">\n",
    "\n",
    "**Expression mathématique de l'algorithme**:\n",
    "\n",
    "Pour un exemple $x^{(i)}$:\n",
    "\n",
    "$$\n",
    "z^{(i)} = w^T x^{(i)} + b \\tag{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i)} = a^{(i)} = \\sigma\\bigl(z^{(i)}\\bigr) \\tag{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}\\bigl(a^{(i)}, y^{(i)}\\bigr) = -\\, y^{(i)} \\log\\bigl(a^{(i)}\\bigr) - \\bigl(1 - y^{(i)}\\bigr) \\log\\bigl(1 - a^{(i)}\\bigr) \\tag{3}\n",
    "$$\n",
    "\n",
    "Le coût est ensuite calculé en sommant sur tous les exemples d'entraînement :\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}\\bigl(a^{(i)}, y^{(i)}\\bigr) \\tag{6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Construction des parties de notre algorithme ## \n",
    "\n",
    "Les principales étapes pour construire un Réseau de Neurones sont :\n",
    "1. Définir la structure du modèle (comme le nombre de caractéristiques en entrée) \n",
    "2. Initialiser les paramètres du modèle\n",
    "3. Boucle :\n",
    "    - Calculer la perte actuelle (propagation avant)\n",
    "    - Calculer le gradient actuel (propagation arrière)\n",
    "    - Mettre à jour les paramètres (descente de gradient)\n",
    "\n",
    "Vous construisez souvent 1-3 séparément et les intégrez dans une seule fonction que nous appelons `model()`.\n",
    "\n",
    "<a name='4-1'></a>\n",
    "### 4.1 - Fonctions auxiliaires\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercice 3 - sigmoid\n",
    "En utilisant votre code des \"Bases de Python\", implémentez `sigmoid()`. Comme vous l'avez vu dans la figure ci-dessus, vous devez calculer $sigmoid(z) = \\frac{1}{1 + e^{-z}}$ pour $z = w^T x + b$ afin de faire des prédictions. Utilisez np.exp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "239ab1cf1028b721fd14f31b8103c40d",
     "grade": false,
     "grade_id": "cell-520521c430352f3b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Calcule la sigmoïde de z\n",
    "\n",
    "    Arguments :\n",
    "    z -- Un scalaire ou un tableau numpy de n'importe quelle taille.\n",
    "\n",
    "    Retour :\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    #(≈ 1 ligne de code)\n",
    "    # s = ...\n",
    "    # VOTRE CODE COMMENCE ICI\n",
    "    \n",
    "    \n",
    "    # VOTRE CODE SE TERMINE ICI\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0483e6820669111a9c5914d8b24bc315",
     "grade": true,
     "grade_id": "cell-30ea3151cab9c491",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))\n",
    "\n",
    "sigmoid_test(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.5, 0, 2.0])\n",
    "output = sigmoid(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 - Initialisation des paramètres\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### Exercice 4 - initialize_with_zeros\n",
    "Implémentez l'initialisation des paramètres dans la cellule ci-dessous. Vous devez initialiser w comme un vecteur de zéros. Si vous ne savez pas quelle fonction numpy utiliser, cherchez np.zeros() dans la documentation de la bibliothèque Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4a37e375a85ddab7274a33abf46bb7c",
     "grade": false,
     "grade_id": "cell-befa9335e479864e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    Cette fonction crée un vecteur de zéros de forme (dim, 1) pour w et initialise b à 0.\n",
    "    \n",
    "    Argument :\n",
    "    dim -- taille du vecteur w que nous voulons (ou nombre de paramètres dans ce cas)\n",
    "    \n",
    "    Retours :\n",
    "    w -- vecteur initialisé de forme (dim, 1)\n",
    "    b -- scalaire initialisé (correspond au biais) de type float\n",
    "    \"\"\"\n",
    "    \n",
    "    # (≈ 2 lignes de code)\n",
    "    # w = ...\n",
    "    # b = ...\n",
    "    # VOTRE CODE COMMENCE ICI\n",
    "    \n",
    "    \n",
    "    # VOTRE CODE SE TERMINE ICI\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4c13b0eafa46ca94de21b41faea8c58",
     "grade": true,
     "grade_id": "cell-a3b6699f145f3a3f",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dim = 2\n",
    "w, b = initialize_with_zeros(dim)\n",
    "\n",
    "assert type(b) == float\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))\n",
    "\n",
    "initialize_with_zeros_test_1(initialize_with_zeros)\n",
    "initialize_with_zeros_test_2(initialize_with_zeros)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-3'></a>\n",
    "### 4.3 - Propagation avant et arrière\n",
    "\n",
    "Maintenant que vos paramètres sont initialisés, vous pouvez effectuer les étapes de propagation \"avant\" et \"arrière\" pour apprendre les paramètres.\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### Exercice 5 - propagate\n",
    "Implémentez une fonction `propagate()` qui calcule la fonction de coût et son gradient.\n",
    "\n",
    "**Indices** :\n",
    "\n",
    "Propagation Avant (Forward Propagation) :\n",
    "- Vous obtenez X\n",
    "- Vous calculez $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- Vous calculez la fonction de coût : $J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))$\n",
    "\n",
    "Voici les deux formules que vous utiliserez : \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8552b2c9cff2b5fa537fab9f98a6e4da",
     "grade": false,
     "grade_id": "cell-11af17e28077b3d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implémente la fonction de coût et son gradient pour la propagation expliquée ci-dessus\n",
    "\n",
    "    Arguments :\n",
    "    w -- poids, un tableau numpy de taille (num_px * num_px * 3, 1)\n",
    "    b -- biais, un scalaire\n",
    "    X -- données de taille (num_px * num_px * 3, nombre d'exemples)\n",
    "    Y -- vecteur des vraies \"étiquettes\" (contenant 0 si non-chat, 1 si chat) de taille (1, nombre d'exemples)\n",
    "\n",
    "    Retour :\n",
    "    grads -- dictionnaire contenant les gradients des poids et du biais\n",
    "            (dw -- gradient de la perte par rapport à w, donc même forme que w)\n",
    "            (db -- gradient de la perte par rapport à b, donc même forme que b)\n",
    "    cost -- coût log-vraisemblance négative pour la régression logistique\n",
    "    \n",
    "    Conseils :\n",
    "    - Écrivez votre code étape par étape pour la propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # PROPAGATION AVANT (DE X AU COÛT)\n",
    "    #(≈ 2 lignes de code)\n",
    "    # calculez l'activation\n",
    "    # A = ...\n",
    "    # calculez le coût en utilisant np.dot pour effectuer la multiplication. \n",
    "    # Et n'utilisez pas de boucles pour la somme.\n",
    "    # cost = ...                                \n",
    "    # VOTRE CODE COMMENCE ICI\n",
    "    \n",
    "    \n",
    "    # VOTRE CODE SE TERMINE ICI\n",
    "\n",
    "    # RÉTROPROPAGATION (POUR TROUVER LE GRAD)\n",
    "    #(≈ 2 lignes de code)\n",
    "    # dw = ...\n",
    "    # db = ...\n",
    "    # VOTRE CODE COMMENCE ICI\n",
    "    \n",
    "    \n",
    "    # VOTRE CODE SE TERMINE ICI\n",
    "    cost = np.squeeze(np.array(cost))\n",
    "\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89373f564dc33ce8a883a55a6ef72b56",
     "grade": true,
     "grade_id": "cell-d1594d75b61dd554",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "w =  np.array([[1.], [2]])\n",
    "b = 1.5\n",
    "\n",
    "# X is using 3 examples, with 2 features each\n",
    "# Each example is stacked column-wise\n",
    "X = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\n",
    "Y = np.array([[1, 1, 0]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "\n",
    "assert type(grads[\"dw\"]) == np.ndarray\n",
    "assert grads[\"dw\"].shape == (2, 1)\n",
    "assert type(grads[\"db\"]) == np.float64\n",
    "\n",
    "\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))\n",
    "\n",
    "propagate_test(propagate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "```\n",
    "dw = [[ 0.25071532]\n",
    " [-0.06604096]]\n",
    "db = -0.1250040450043965\n",
    "cost = 0.15900537707692405\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-4'></a>\n",
    "### 4.4 - Optimisation\n",
    "- Vous avez initialisé vos paramètres.\n",
    "- Vous êtes également capable de calculer une fonction de coût et son gradient.\n",
    "- Maintenant, vous voulez mettre à jour les paramètres en utilisant la descente de gradient.\n",
    "\n",
    "<a name='ex-6'></a>\n",
    "### Exercice 6 - optimize\n",
    "Écrivez la fonction d'optimisation. Le but est d'apprendre $w$ et $b$ en minimisant la fonction de coût $J$. Pour un paramètre $\\theta$, la règle de mise à jour est $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, où $\\alpha$ est le taux d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49d9b4c1a780bf141c8eb48e06cbb494",
     "grade": false,
     "grade_id": "cell-616d6883e807448d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n",
    "    \"\"\"\n",
    "    Cette fonction optimise w et b en exécutant un algorithme de descente de gradient\n",
    "    \n",
    "    Arguments :\n",
    "    w -- poids, un tableau numpy de taille (num_px * num_px * 3, 1)\n",
    "    b -- biais, un scalaire\n",
    "    X -- données de forme (num_px * num_px * 3, nombre d'exemples)\n",
    "    Y -- vecteur des vraies \"étiquettes\" (0 si non-chat, 1 si chat), de forme (1, nombre d'exemples)\n",
    "    num_iterations -- nombre d'itérations de la boucle d'optimisation\n",
    "    learning_rate -- taux d'apprentissage de la règle de mise à jour de la descente de gradient\n",
    "    print_cost -- True pour afficher la perte tous les 100 pas\n",
    "    \n",
    "    Retours :\n",
    "    params -- dictionnaire contenant les poids w et le biais b\n",
    "    grads -- dictionnaire contenant les gradients des poids et du biais par rapport à la fonction de coût\n",
    "    costs -- liste de tous les coûts calculés pendant l'optimisation, cela sera utilisé pour tracer la courbe d'apprentissage.\n",
    "    \n",
    "    Conseils :\n",
    "    Vous devez fondamentalement écrire deux étapes et itérer à travers elles :\n",
    "        1) Calculer le coût et le gradient pour les paramètres actuels. Utilisez propagate().\n",
    "        2) Mettre à jour les paramètres en utilisant la règle de descente de gradient pour w et b.\n",
    "    \"\"\"\n",
    "    \n",
    "    w = copy.deepcopy(w)\n",
    "    b = copy.deepcopy(b)\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # (≈ 1 ligne de code)\n",
    "        # Calcul du coût et du gradient \n",
    "        # grads, cost = ...\n",
    "        # VOTRE CODE COMMENCE ICI\n",
    "        \n",
    "        \n",
    "        # VOTRE CODE SE TERMINE ICI\n",
    "        \n",
    "        # Récupérer les dérivées depuis grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # Règle de mise à jour (≈ 2 lignes de code)\n",
    "        # w = ...\n",
    "        # b = ...\n",
    "        # VOTRE CODE COMMENCE ICI\n",
    "        \n",
    "        \n",
    "        # VOTRE CODE SE TERMINE ICI\n",
    "        \n",
    "        # Enregistrer les coûts\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "            # Afficher le coût toutes les 100 itérations d'entraînement\n",
    "            if print_cost:\n",
    "                print (\"Coût après l'itération %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b65a5c90f86a990614156e41f64b4678",
     "grade": true,
     "grade_id": "cell-8e3d43fbb82a8901",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "params, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print(\"Costs = \" + str(costs))\n",
    "\n",
    "optimize_test(optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-7'></a>\n",
    "### Exercice 7 - predict\n",
    "La fonction précédente produira les paramètres w et b appris. Nous pouvons utiliser w et b pour prédire les étiquettes pour un jeu de données X. Implémentez la fonction `predict()`. Il y a deux étapes pour calculer les prédictions :\n",
    "\n",
    "1. Calculer $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. Convertir les entrées de a en 0 (si activation <= 0.5) ou 1 (si activation > 0.5), stocker les prédictions dans un vecteur `Y_prediction`. Si vous le souhaitez, vous pouvez utiliser une instruction `if`/`else` dans une boucle `for` (bien qu'il existe aussi un moyen de vectoriser cela). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e56419b97ebf382a8f93ac2873988887",
     "grade": false,
     "grade_id": "cell-d6f924f49c51dc2f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Prédire si l'étiquette est 0 ou 1 en utilisant les paramètres de régression logistique appris (w, b)\n",
    "    \n",
    "    Arguments :\n",
    "    w -- poids, un tableau numpy de taille (num_px * num_px * 3, 1)\n",
    "    b -- biais, un scalaire\n",
    "    X -- données de taille (num_px * num_px * 3, nombre d'exemples)\n",
    "    \n",
    "    Retours :\n",
    "    Y_prediction -- un tableau numpy (vecteur) contenant toutes les prédictions (0/1) pour les exemples dans X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Calculer le vecteur \"A\" prédisant les probabilités de la présence d'un chat dans l'image\n",
    "    #(≈ 1 ligne de code)\n",
    "    # A = ...\n",
    "    # VOTRE CODE COMMENCE ICI\n",
    "    \n",
    "    \n",
    "    # VOTRE CODE SE TERMINE ICI\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convertir les probabilités A[0,i] en prédictions réelles Y_prediction[0,i]\n",
    "        #(≈ 4 lignes de code)\n",
    "        # if A[0, i] > ____ :\n",
    "        #     Y_prediction[0,i] = \n",
    "        # else:\n",
    "        #     Y_prediction[0,i] = \n",
    "        # VOTRE CODE COMMENCE ICI\n",
    "        \n",
    "        \n",
    "        # VOTRE CODE SE TERMINE ICI\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3ea12608f15798d542a07c1bc9f561b",
     "grade": true,
     "grade_id": "cell-90b1fb967269548c",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "w = np.array([[0.1124579], [0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))\n",
    "\n",
    "predict_test(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**Ce qu'il faut retenir :**\n",
    "    \n",
    "Vous avez implémenté plusieurs fonctions qui :\n",
    "- Initialisent (w,b)\n",
    "- Optimisent la perte itérativement pour apprendre les paramètres (w,b) :\n",
    "    - Calcul du coût et de son gradient \n",
    "    - Mise à jour des paramètres par descente de gradient\n",
    "- Utilisent les (w,b) appris pour prédire les étiquettes pour un ensemble d'exemples donné"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Fusionner toutes les fonctions dans un modèle ##\n",
    "\n",
    "Vous allez maintenant voir comment le modèle global est structuré en rassemblant tous les blocs de construction (fonctions implémentées dans les parties précédentes) ensemble, dans le bon ordre.\n",
    "\n",
    "<a name='ex-8'></a>\n",
    "### Exercice 8 - model\n",
    "Implémentez la fonction modèle. Utilisez la notation suivante :\n",
    "    - Y_prediction_test pour vos prédictions sur l'ensemble de test\n",
    "    - Y_prediction_train pour vos prédictions sur l'ensemble d'entraînement\n",
    "    - parameters, grads, costs pour les sorties de optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b62adfb8f5a0f5bb5aa6798c3c5df66d",
     "grade": false,
     "grade_id": "cell-6dcba5967c4cbf8c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    Construit le modèle de régression logistique en appelant la fonction que vous avez implémentée précédemment\n",
    "    \n",
    "    Arguments :\n",
    "    X_train -- ensemble d'entraînement représenté par un tableau numpy de forme (num_px * num_px * 3, m_train)\n",
    "    Y_train -- étiquettes d'entraînement représentées par un tableau numpy (vecteur) de forme (1, m_train)\n",
    "    X_test -- ensemble de test représenté par un tableau numpy de forme (num_px * num_px * 3, m_test)\n",
    "    Y_test -- étiquettes de test représentées par un tableau numpy (vecteur) de forme (1, m_test)\n",
    "    num_iterations -- hyperparamètre représentant le nombre d'itérations pour optimiser les paramètres\n",
    "    learning_rate -- hyperparamètre représentant le taux d'apprentissage utilisé dans la règle de mise à jour de optimize()\n",
    "    print_cost -- Mettre à True pour afficher le coût toutes les 100 itérations\n",
    "    \n",
    "    Retours :\n",
    "    d -- dictionnaire contenant des informations sur le modèle.\n",
    "    \"\"\"\n",
    "    # (≈ 1 ligne de code)   \n",
    "    # initialiser les paramètres avec des zéros\n",
    "    # et utilisez la fonction \"shape\" pour obtenir la première dimension de X_train\n",
    "    # w, b = ...\n",
    "    \n",
    "    #(≈ 1 ligne de code)\n",
    "    # Descente de gradient \n",
    "    # params, grads, costs = ...\n",
    "    \n",
    "    # Récupérer les paramètres w et b depuis le dictionnaire \"params\"\n",
    "    # w = ...\n",
    "    # b = ...\n",
    "    \n",
    "    # Prédire les exemples de l'ensemble de test/train (≈ 2 lignes de code)\n",
    "    # Y_prediction_test = ...\n",
    "    # Y_prediction_train = ...\n",
    "    \n",
    "    # VOTRE CODE COMMENCE ICI\n",
    "    \n",
    "    \n",
    "    # VOTRE CODE SE TERMINE ICI\n",
    "\n",
    "    # Afficher les erreurs d'entraînement/test\n",
    "    if print_cost:\n",
    "        print(\"précision train : {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "        print(\"précision test : {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b468bc5ddf6ecc5c7dbcb9a02cfe0216",
     "grade": true,
     "grade_id": "cell-4170e070f3cde17e",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from public_tests import *\n",
    "\n",
    "model_test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous réussissez tous les tests, exécutez la cellule suivante pour entraîner votre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commentaire** : La précision d'entraînement est proche de 100 %. C'est une bonne vérification de cohérence : votre modèle fonctionne et a une capacité suffisante pour s'adapter aux données d'entraînement. La précision de test est de 70 %. Ce n'est en fait pas mal pour ce modèle simple, étant donné le petit jeu de données que nous avons utilisé et le fait que la régression logistique est un classifieur linéaire. \n",
    "\n",
    "Aussi, vous voyez que le modèle fait clairement du surapprentissage (overfitting) sur les données d'entraînement. Plus tard dans cette spécialisation, vous apprendrez comment réduire le surapprentissage, par exemple en utilisant la régularisation. En utilisant le code ci-dessous (et en changeant la variable `index`), vous pouvez regarder les prédictions sur les images de l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'une image mal classée.\n",
    "index = 1\n",
    "plt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n",
    "print (\"y = \" + str(test_set_y[0,index]) + \", vous avez prédit que c'est une image de \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laissez-nous aussi tracer la fonction de coût et les gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracer la courbe d'apprentissage (avec les coûts)\n",
    "costs = np.squeeze(logistic_regression_model['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('coût')\n",
    "plt.xlabel('itérations (par centaines)')\n",
    "plt.title(\"Taux d'apprentissage =\" + str(logistic_regression_model[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation** :\n",
    "Vous pouvez voir le coût diminuer. Cela montre que les paramètres sont en train d'être appris. Cependant, vous voyez que vous pourriez entraîner le modèle encore plus sur l'ensemble d'entraînement. Essayez d'augmenter le nombre d'itérations dans la cellule ci-dessus et relancez les cellules. Vous pourriez voir que la précision de l'ensemble d'entraînement augmente, mais que la précision de l'ensemble de test diminue. C'est ce qu'on appelle le surapprentissage (overfitting). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Analyse approfondie (optionnel/non noté) ##\n",
    "\n",
    "Félicitations pour avoir construit votre premier modèle de classification d'images. Analysons-le davantage et examinons les choix possibles pour le taux d'apprentissage $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choix du taux d'apprentissage ####\n",
    "\n",
    "**Rappel** :\n",
    "Pour que la Descente de Gradient fonctionne, vous devez choisir le taux d'apprentissage judicieusement. Le taux d'apprentissage $\\alpha$ détermine à quelle vitesse nous mettons à jour les paramètres. Si le taux d'apprentissage est trop grand, nous pouvons \"dépasser\" la valeur optimale. De même, s'il est trop petit, nous aurons besoin de trop d'itérations pour converger vers les meilleures valeurs. C'est pourquoi il est crucial d'utiliser un taux d'apprentissage bien ajusté.\n",
    "\n",
    "Comparons la courbe d'apprentissage de notre modèle avec plusieurs choix de taux d'apprentissage. Exécutez la cellule ci-dessous. Cela devrait prendre environ 1 minute. N'hésitez pas à essayer d'autres valeurs que les trois que nous avons initialisées dans la variable `learning_rates` et voyez ce qui se passe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "models = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print (\"Entraînement d'un modèle avec taux d'apprentissage : \" + str(lr))\n",
    "    models[str(lr)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=1500, learning_rate=lr, print_cost=False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for lr in learning_rates:\n",
    "    plt.plot(np.squeeze(models[str(lr)][\"costs\"]), label=str(models[str(lr)][\"learning_rate\"]))\n",
    "\n",
    "plt.ylabel('coût')\n",
    "plt.xlabel('itérations (centaines)')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation** : \n",
    "- Différents taux d'apprentissage donnent différents coûts et donc différents résultats de prédictions.\n",
    "- Si le taux d'apprentissage est trop grand (0.01), le coût peut osciller de haut en bas. Il peut même diverger (bien que dans cet exemple, l'utilisation de 0.01 finisse quand même par donner une bonne valeur pour le coût). \n",
    "- Un coût plus faible ne signifie pas un meilleur modèle. Vous devez vérifier s'il y a possiblement du surapprentissage. Cela se produit lorsque la précision d'entraînement est beaucoup plus élevée que la précision de test.\n",
    "- En deep learning, nous recommandons généralement de : \n",
    "    - Choisir le taux d'apprentissage qui minimise le mieux la fonction de coût.\n",
    "    - Si votre modèle surapprend, utilisez d'autres techniques pour réduire le surapprentissage. (Nous en parlerons dans des vidéos ultérieures.) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Test avec votre propre image (optionnel/non noté) ##\n",
    "\n",
    "Félicitations pour avoir terminé ce devoir. Vous pouvez utiliser votre propre image et voir la sortie de votre modèle. Pour ce faire :\n",
    "    1. Cliquez sur \"File\" dans la barre supérieure de ce notebook, puis cliquez sur \"Open\" pour aller sur votre Coursera Hub.\n",
    "    2. Ajoutez votre image dans le répertoire de ce Jupyter Notebook, dans le dossier \"images\"\n",
    "    3. Changez le nom de votre image dans le code suivant\n",
    "    4. Exécutez le code et vérifiez si l'algorithme a raison (1 = chat, 0 = non-chat) !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changez ceci par le nom de votre fichier image\n",
    "my_image = \"my_image.jpg\"   \n",
    "\n",
    "# Nous prétraitons l'image pour qu'elle s'adapte à votre algorithme.\n",
    "fname = \"images/\" + my_image\n",
    "image = np.array(Image.open(fname).resize((num_px, num_px)))\n",
    "plt.imshow(image)\n",
    "image = image / 255.\n",
    "image = image.reshape((1, num_px * num_px * 3)).T\n",
    "my_predicted_image = predict(logistic_regression_model[\"w\"], logistic_regression_model[\"b\"], image)\n",
    "\n",
    "print(\"y = \" + str(np.squeeze(my_predicted_image)) + \", votre algorithme prédit que c'est une image de \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**Ce qu'il faut retenir de ce devoir :**\n",
    "1. Le prétraitement du jeu de données est important.\n",
    "2. Vous avez implémenté chaque fonction séparément : initialize(), propagate(), optimize(). Ensuite, vous avez construit un model().\n",
    "3. Le réglage du taux d'apprentissage (qui est un exemple d'\"hyperparamètre\") peut faire une grande différence pour l'algorithme. Vous verrez plus d'exemples de cela plus tard dans ce cours !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, si vous le souhaitez, nous vous invitons à essayer différentes choses sur ce Notebook. Assurez-vous de soumettre avant d'essayer quoi que ce soit. Une fois que vous avez soumis, les choses avec lesquelles vous pouvez jouer incluent :\n",
    "    - Jouer avec le taux d'apprentissage et le nombre d'itérations\n",
    "    - Essayer différentes méthodes d'initialisation et comparer les résultats\n",
    "    - Tester d'autres prétraitements (centrer les données, ou diviser chaque ligne par son écart type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliographie :\n",
    "- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n",
    "- https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
